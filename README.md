# UofT degree mapper
 An (unofficial) University of Toronto degree mapper, made as a project for the CSC111 course by Alex Macri, Alan Mirzoev, Lasanda Induwara Dewamuni Mudalige, and Padina Nasiri Toussi.

# Computational Overview
We had a class called Node that would represent the vertices of the graph, and would contain the information of either a course or a program. Basic functions were created to check if it was a program and to check if a course was in another course's prerequisites, exclusions, and or dictionaries, or to return those dictionaries in general.
This class would be imported into the file with the University class, which functions analogous to our graph class from lecture. This contains two attributes: a dictionary of nodes which would contain all the classes and programs we would use, and edges, which would be a list of tuples that would be used to generate the graph by connecting a course to its prerequisite. Some helper functions were made for this class, such as 'get\_num\_credits', which gets the amount of credits a course gives by looking at its course code. The two main methods in the University class are 'edges\_to\_tuple', and 'translate\_data' (and its helper function, 'scrapy\_to\_valid\_input'). The former is a recursive graph traversal function. It traverses through each node's prerequisites, and if the prerequisite isn't in the set we use to keep track of the visited prerequisites, we add it to that set and create an edge (a tuple with both ends of the edge, as defined by networkx) with the prerequisite and the current node. After that, we check to see if the current prerequisite is in the prerequisite's list of prerequisites (to prevent us from infinitely recursing), and if not, then we we recurse with the prerequisite (thus going through their prerequisites and making edges). The latter functions I mentioned earlier are for turning the scraped data into nodes. The helper function, 'scrapy\_to\_valid\_input', is called first, and it uses helper functions to add values to the data in the scraped node dictionaries that doesn't need to be scraped itself (like the amount of credits you get from taking the course. We can just see if it's a one-semester course or two-semester course and get the amount of credits from there). After calling that function on the raw, just-scraped data, the 'translate\_data' function loops through the list of dictionaries for each node and creates each actual node object (without linking them to each other) with the values specified in the node dictionaries. Then, there's another loop through the node dictionaries that connects the current node with its prerequisites, and goes through the prerequisites to see where two adjacent prerequisite nodes are "or for it" (indicating that you just have to take one of them to be able to take the current course), and then adding each other to their 'or\_' attribute dictionaries in a list, with the current node's name as the key.
To collect the data for this project, we used the Scrapy library for Python. This library allowed us to easily set up web scraping, which allows us to automatically go through webpages and collect certain pieces of text from each webpage, going off link extraction rules and css selectors defined in our subclass of 'CrawlSpider' (which is in the file 'courses.py'). This subclass essentially lets us define where and how we traverse a domain for scraping, and how we parse the results. The 'CourseCrawler' class' 'parse' method is how we parse each website we visit, and since we return the results in a dict, we're able to save the parsed outputs of our web crawling and scraping into a json file with a certain command (specified in the next section).
We also display our gui with tkinter, which is where you select your major and see course information, and use a pop-out window of networkx to display the graph of courses you need to take. Since tkinter and networkx make it fairly simple/formulaic to generate their graphics, there isn't too much to mention here, except for the helper function 'create\_edge\_color\_list', which generates a list of colours for edges based on if you need to take one OR another (same-colour edge), or one AND another (black edge).

# Overview of used Datasets
We used U of T's Arts and Science calendar as our source for the course and program data (Course Search). We scraped the course search page which is a list of all offered courses, and includes details like prerequisites, exclusions, number of credits and breadth. We also used the program search page, to get the names and prerequisites of different programs (Program Search). We used the above information to create our own datasets stored as json files: courses.json and programs.json. 
The courses dataset is a list of dictionaries, where each element of the list represents a single course. In each course dictionary, there are 4 keys: name, breadth, prerequisites and exclusions. The key name is associated with the full name of the course and breadth is associated with the breadth name. The keys prerequisites and exclusions each associate to a list containing the course's prerequisites or exclusions. 
The programs dataset is a list of dictionaries with each element representing a single program. It has a very similar format to the courses dataset as each dictionary also includes the same 4 keys of name, breadth, prerequisites and exclusions, with the same function. Except that in this dataset, breadth and exclusions are both set to null.
